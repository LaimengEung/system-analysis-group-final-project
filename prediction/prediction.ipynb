{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dbe80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40313326",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'synthetic_ride_hail_phnom_penh.csv'\n",
    "\n",
    "data = pd.read_csv(PATH)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77e12da",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901936c",
   "metadata": {},
   "source": [
    "### Label Encode for Ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eb62b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic_level has a natural order\n",
    "traffic_order = {'Low': 1, 'Medium': 2, 'High': 3, 'Very High': 4}\n",
    "data['traffic_level_encoded'] = data['traffic_level'].map(traffic_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7be92",
   "metadata": {},
   "source": [
    "### One-Hot Encode for Norminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce858a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Nominal (One-Hot) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "data = pd.get_dummies(data, columns=['vehicle_type', 'weather', 'time_of_day'],\n",
    "                    drop_first=True)\n",
    "\n",
    "# â”€â”€ Date/Time Features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "data['hour']        = pd.to_datetime(data['request_time']).dt.hour\n",
    "data['date']        = pd.to_datetime(data['date'])\n",
    "data['day_of_week'] = data['date'].dt.dayofweek   # 0â€“6 (label encode naturally)\n",
    "data['is_weekday']  = (data['day_of_week'] < 5).astype(int)\n",
    "# âŒ Skip is_weekend (redundant with is_weekday)\n",
    "\n",
    "# â”€â”€ Drop originals + non-numeric string columns â”€â”€\n",
    "data.drop(columns=[\n",
    "    'traffic_level',\n",
    "    'date',\n",
    "    'request_time',\n",
    "    'arr_time',       # â† this is the culprit '12:15'\n",
    "    'trip_id',        # â† ID, not a feature\n",
    "    'dept_lat', 'dept_lon', 'arr_lat', 'arr_lon',  # â† coordinates (optional)\n",
    "    'day',            # â† already have day_of_week\n",
    "], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert any remaining bool columns to int (some sklearn versions don't like bool)\n",
    "# Turn bool into int\n",
    "bool_cols = data.select_dtypes(include='bool').columns\n",
    "data[bool_cols] = data[bool_cols].astype(int)\n",
    "\n",
    "# Final check â€” should print nothing if all clear\n",
    "print(\"Remaining non-numeric columns:\", \n",
    "      data.select_dtypes(exclude='number').columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491c61b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404aa0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d419c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'fare_usd'     # we want to predict the fare_usd\n",
    "\n",
    "X = data.drop(columns=[target])\n",
    "y = data[target]\n",
    "\n",
    "print(\"Shape of Features:\", X.shape)\n",
    "print(\"Shape of Target:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ebc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Shape of X Train:\", X_train.shape)\n",
    "print(\"Shape of X Test:\", X_test.shape)\n",
    "print(\"Shape of y Train\", y_train.shape)\n",
    "print(\"Shape of y Test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01611fa4",
   "metadata": {},
   "source": [
    "### Standard Scaled our Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060953f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this BEFORE scaling to inspect remaining string columns\n",
    "print(X_train.dtypes[X_train.dtypes == 'object'])\n",
    "print(X_train.dtypes[X_train.dtypes == 'category'])  # time_of_day might still be category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a612b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Linear Regression\":    LinearRegression(),\n",
    "    \"Decision Tree\":        DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest\":        RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d840a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"  MAE  : {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "    print(f\"  RMSE : {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
    "    print(f\"  RÂ²   : {r2_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.linear_model    import LinearRegression\n",
    "from sklearn.tree            import DecisionTreeRegressor\n",
    "from sklearn.ensemble        import RandomForestRegressor\n",
    "import xgboost                as xgb\n",
    "import numpy                  as np\n",
    "\n",
    "# â”€â”€ Cross-Validation Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# â”€â”€ Hyperparameter Grids â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "param_grids = {\n",
    "    \"Linear Regression\": {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {\n",
    "            'fit_intercept': [True, False],\n",
    "            'positive':      [True, False],\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"Decision Tree\": {\n",
    "        'model': DecisionTreeRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'max_depth':        [3, 5, 10, None],\n",
    "            'min_samples_split':[2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features':     ['sqrt', 'log2', None],\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"Random Forest\": {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators':     [50, 100, 200],\n",
    "            'max_depth':        [3, 5, 10, None],\n",
    "            'min_samples_split':[2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features':     ['sqrt', 'log2'],\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"XGBoost\": {\n",
    "        'model': xgb.XGBRegressor(random_state=42, verbosity=0),\n",
    "        'params': {\n",
    "            'n_estimators':     [50, 100, 200],\n",
    "            'max_depth':        [3, 5, 7],\n",
    "            'learning_rate':    [0.01, 0.05, 0.1, 0.2],\n",
    "            'subsample':        [0.7, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "            'reg_alpha':        [0, 0.1, 0.5],   # L1 regularization\n",
    "            'reg_lambda':       [1, 1.5, 2.0],   # L2 regularization\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# â”€â”€ Run GridSearchCV for Each Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best_models    = {}\n",
    "cv_results_all = {}\n",
    "\n",
    "for name, config in param_grids.items():\n",
    "    print(f\"\\nðŸ” Tuning: {name} ...\")\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator  = config['model'],\n",
    "        param_grid = config['params'],\n",
    "        cv         = kf,\n",
    "        scoring    = 'neg_mean_absolute_error',  # MAE as scoring metric\n",
    "        n_jobs     = -1,                          # use all CPU cores\n",
    "        verbose    = 1,\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    best_models[name]    = grid_search.best_estimator_\n",
    "    cv_results_all[name] = grid_search\n",
    "\n",
    "    print(f\"   âœ… Best Params : {grid_search.best_params_}\")\n",
    "    print(f\"   âœ… Best CV MAE : ${-grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd8b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Evaluate All Tuned Models on Test Set â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ“Š TUNED MODEL PERFORMANCE ON TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    mae  = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Custom tolerance accuracy\n",
    "    acc_050 = np.mean(np.abs(y_test.values - y_pred) <= 0.50) * 100\n",
    "    acc_100 = np.mean(np.abs(y_test.values - y_pred) <= 1.00) * 100\n",
    "\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'MAE':   mae,\n",
    "        'RMSE':  rmse,\n",
    "        'RÂ²':    r2,\n",
    "        'AccÂ±$0.50': acc_050,\n",
    "        'AccÂ±$1.00': acc_100,\n",
    "    })\n",
    "\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(f\"   MAE        : ${mae:.4f}\")\n",
    "    print(f\"   RMSE       : ${rmse:.4f}\")\n",
    "    print(f\"   RÂ²         : {r2:.4f}\")\n",
    "    print(f\"   Acc Â±$0.50 : {acc_050:.1f}%\")\n",
    "    print(f\"   Acc Â±$1.00 : {acc_100:.1f}%\")\n",
    "\n",
    "# â”€â”€ Results Summary Table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "results_df = pd.DataFrame(results).set_index('Model')\n",
    "print(\"\\n\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_model = models[\"Random Forest\"]\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()],\n",
    "         [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual fare_usd\")\n",
    "plt.ylabel(\"Predicted fare_usd\")\n",
    "plt.title(\"Actual vs Predicted Fare\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "importances = best_model.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "sns.barplot(x=feat_imp.values[:10], y=feat_imp.index[:10])\n",
    "plt.title(\"Top 10 Feature Importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a924259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction on test set using the best model\n",
    "y_pred_test = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Convert scaled features back to DataFrame for display\n",
    "X_test_display = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Create a detailed results DataFrame\n",
    "detailed_results = X_test_display.copy()\n",
    "detailed_results['Actual_Fare']    = y_test.values\n",
    "detailed_results['Predicted_Fare'] = y_pred_test\n",
    "detailed_results['Error']          = abs(y_test.values - y_pred_test)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"DETAILED FARE PREDICTIONS ON TEST SET\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nShowing key features that influence each prediction:\\n\")\n",
    "\n",
    "# Display first 5 detailed predictions\n",
    "for i in range(min(5, len(detailed_results))):\n",
    "    row = detailed_results.iloc[i]\n",
    "\n",
    "    print(f\"\\n{'â”€' * 100}\")\n",
    "    print(f\"ðŸ›º PREDICTION #{i+1}\")\n",
    "    print(f\"{'â”€' * 100}\")\n",
    "\n",
    "    print(f\"\\nðŸ“ Trip Details:\")\n",
    "    print(f\"   Trip Distance:      {row['trip_distance_km']:.2f} (scaled)\")\n",
    "    print(f\"   Actual Time:        {row['actual_time_min']:.2f} (scaled)\")\n",
    "    print(f\"   Estimated Time:     {row['est_time_min']:.2f} (scaled)\")\n",
    "    print(f\"   Wait Time:          {row['wait_time_min']:.2f} (scaled)\")\n",
    "\n",
    "    print(f\"\\nðŸŒ¤ï¸  Conditions:\")\n",
    "    print(f\"   Traffic Level:      {row['traffic_level_encoded']:.2f} (encoded & scaled)\")\n",
    "\n",
    "    print(f\"\\nðŸ’° Pricing:\")\n",
    "    print(f\"   Surge Pricing:      {row['surge_pricing']:.2f} (scaled)\")\n",
    "    print(f\"   Tip (USD):          {row['tip_usd']:.2f} (scaled)\")\n",
    "\n",
    "    print(f\"\\nâ­ Driver:\")\n",
    "    print(f\"   Driver Rating:      {row['driver_rating']:.2f} (scaled)\")\n",
    "\n",
    "    print(f\"\\nðŸ“… Timing:\")\n",
    "    print(f\"   Hour:               {row['hour']:.2f}\")\n",
    "    print(f\"   Day of Week:        {row['day_of_week']:.2f}\")\n",
    "    print(f\"   Is Weekday:         {row['is_weekday']:.2f}\")\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ PREDICTION RESULTS:\")\n",
    "    print(f\"   Actual Fare:        ${row['Actual_Fare']:.2f}\")\n",
    "    print(f\"   Predicted Fare:     ${row['Predicted_Fare']:.2f}\")\n",
    "    print(f\"   Error:              ${row['Error']:.2f}\")\n",
    "\n",
    "    # Accuracy indicator based on fare error\n",
    "    if row['Error'] < 0.50:\n",
    "        accuracy = \"ðŸŸ¢ Excellent\"\n",
    "    elif row['Error'] < 1.00:\n",
    "        accuracy = \"ðŸŸ¡ Good\"\n",
    "    elif row['Error'] < 2.00:\n",
    "        accuracy = \"ðŸŸ  Fair\"\n",
    "    else:\n",
    "        accuracy = \"ðŸ”´ Poor\"\n",
    "    print(f\"   Accuracy:           {accuracy}\")\n",
    "\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "\n",
    "# â”€â”€ Summary Statistics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nðŸ“Š OVERALL TEST SET PERFORMANCE\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Total Samples:                {len(y_test)}\")\n",
    "print(f\"Mean Actual Fare:             ${y_test.mean():.2f}\")\n",
    "print(f\"Mean Predicted Fare:          ${y_pred_test.mean():.2f}\")\n",
    "print(f\"\\nMean Absolute Error:          ${mean_absolute_error(y_test, y_pred_test):.4f}\")\n",
    "print(f\"Root Mean Squared Error:      ${np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}\")\n",
    "print(f\"RÂ² Score:                     {r2_score(y_test, y_pred_test):.4f}\")\n",
    "\n",
    "# â”€â”€ Error Distribution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "errors = abs(y_test.values - y_pred_test)\n",
    "print(f\"\\nðŸ“ˆ ERROR DISTRIBUTION:\")\n",
    "print(f\"   Excellent (< $0.50):  {sum(errors < 0.50)} samples ({100*sum(errors < 0.50)/len(errors):.1f}%)\")\n",
    "print(f\"   Good      ($0.50â€“$1): {sum((errors >= 0.50) & (errors < 1.00))} samples ({100*sum((errors >= 0.50) & (errors < 1.00))/len(errors):.1f}%)\")\n",
    "print(f\"   Fair      ($1â€“$2):    {sum((errors >= 1.00) & (errors < 2.00))} samples ({100*sum((errors >= 1.00) & (errors < 2.00))/len(errors):.1f}%)\")\n",
    "print(f\"   Poor      (> $2):     {sum(errors >= 2.00)} samples ({100*sum(errors >= 2.00)/len(errors):.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
